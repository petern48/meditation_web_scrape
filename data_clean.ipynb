{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large\n",
    "# !pip install deepmultilingualpunctuation protobuf\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSIGHT_DATA_FILE = 'insight-transcripts-data.csv'\n",
    "INSIGHT_DATA_FILE = 'insight-transcripts-data-cleaned.csv'\n",
    "YOUTUBE_DATA_FILE = 'yt-transcripts-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PunctuationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common intros to remove\n",
    "PHRASES_TO_REMOVE = [\n",
    "    r'.*welcome to carries conscious living are you ready to',\n",
    "    r'.*on the 7th of this month are you ready to',\n",
    "    r'.*are (you|we) ready to meditate (with|the)',\n",
    "    r'.*join the patreon the links are in the description',\n",
    "    r'.*if you are returning welcome back here at the',\n",
    "    r\".*I'm Sara Raymond here at the mindful movement\",\n",
    "    r'.*your host Brian Scott',\n",
    "    r'.*create your free Mindvalley account today at mindvalley.com'\n",
    "]\n",
    "\n",
    "def data_clean(csv_file_path):\n",
    "    cleaned_file_name = csv_file_path.rstrip('.csv')\n",
    "    cleaned_file_name += '-cleaned.csv'\n",
    "\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf8')\n",
    "    # df.dropna()\n",
    "\n",
    "    with open(cleaned_file_name, 'a', encoding='utf8', newline='') as csvf:\n",
    "        writer = csv.writer(csvf)\n",
    "        # if csvf empty\n",
    "        # writer.writerow(['Meditation_Type','URL','Script'])\n",
    "        for idx, item in df.iterrows():\n",
    "            s = item['Script']\n",
    "            s = re.sub('\\[.*?\\]', ' ', s)  # Remove content inside square brackets\n",
    "            s = re.sub('\\(.*?\\)', ' ', s)  # Remove contents inside square brackets\n",
    "            s = re.sub('\\s+',' ', s)  # Replace consecutive whitespace with a single space\n",
    "\n",
    "            # Remove specific phrases  (FOR youtube data)\n",
    "            s = re.sub('so( so)+', 'so', s)  # Remove consecutive 'so'\n",
    "            s = re.sub('foreign( foreign)+', 'foreign', s)  # Remove consecutive 'foreign' \n",
    "            s = re.sub('you( you)+', 'you', s)  # Remove consecutive 'you'\n",
    "\n",
    "            # Remove the unrelated introductions\n",
    "            s = re.sub(\".*let's (begin|start)\", \"let's start\", s)\n",
    "            s = re.sub(\".*(begin|start) by\", 'start by', s)\n",
    "            for phrase in PHRASES_TO_REMOVE:\n",
    "                s = re.sub(phrase, '', s)\n",
    "\n",
    "            # Add Punctuation\n",
    "            s = model.restore_punctuation(s)  # Takes very long time\n",
    "\n",
    "            script = s\n",
    "            url = item['URL']\n",
    "            med_type = item['Meditation_Type']\n",
    "            writer.writerow([med_type, url, script])\n",
    "            print(f\"Done Line {idx + 2}\")  # Account for 0 index and the header line\n",
    "\n",
    "    print(f'Written cleaned data to {cleaned_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean(YOUTUBE_DATA_FILE)  # ADD -cleaned file??m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean(INSIGHT_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding punctuation, grep for lines that don't end with period\n",
    "\n",
    "# Remove all instances of . you. and you\" using find and replace (regex)\n",
    "# Do the same with foreign  (also very common in youtube scripts)\n",
    "# (youtube transcripts have a bunch of random you.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created new file `med-transcript-dataset.csv` with the insight timer data followed by the youtube data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add special tokens to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = 'med-transcript-dataset.csv'\n",
    "TOKENIZED_DATA_FILE = 'tokenized-dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv(INPUT_DATA_FILE, encoding=\"utf8\")  # encoding=\"ISO-8859-1\"  Original\n",
    "# df = df.dropna()  # DO NOT dropna() because insight timer rows have nan urls\n",
    "# Empty file\n",
    "with open(TOKENIZED_DATA_FILE, 'w') as f:\n",
    "    f.truncate(0)\n",
    "\n",
    "with open(TOKENIZED_DATA_FILE, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Meditation_Type','URL','Script'])\n",
    "    for idx, item in df.iterrows():\n",
    "        script = item['Script']\n",
    "        url = item['URL']\n",
    "        med_type = item['Meditation_Type']\n",
    "\n",
    "        # ADD SPECIAL TOKEN indicating the meditation type\n",
    "        if type(med_type)==str:\n",
    "            special_token = f'[{med_type} MEDITATION]'.upper()\n",
    "        # If nan values (type float), then don't add token\n",
    "        else:\n",
    "            special_token = \"\"\n",
    "\n",
    "        script = f'{special_token} {script}'\n",
    "\n",
    "        writer.writerow([med_type, url, script])\n",
    "\n",
    "print(f\"Written tokenized data file to {TOKENIZED_DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze what the most common words to find phrases we should remove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
